# src/core/collectors.py
import asyncio
import httpx
import time
import traceback  # [æ–°å¢] ç”¨äºæ‰“å°è¯¦ç»†æŠ¥é”™å †æ ˆ
from typing import Set, List, Dict, Any
from datetime import datetime, timezone, timedelta

# å¯¼å…¥å¯ä»¥ç›´æ¥è°ƒç”¨çš„ç»„ä»¶
from src.agents.small_agents.pipeline import small_agent_graph
from src.schemas.data_models import RawDataInput

# --- é…ç½® ---
FETCH_API_URL = "http://api.ibyteai.com:15008/10Ai/dataCenter/crypto/fetchCryptoPanic"
# [æ–°å¢] æ›´æ–°æ¥å£ï¼Œç”¨äºæ ‡è®°å¤„ç†å¤±è´¥çš„æ–°é—»
UPDATE_API_URL = "http://api.ibyteai.com:15008/10Ai/dataCenter/crypto/updatePanicNews"
HEADERS = {'Content-Type': 'application/json'}

NEWS_POLL_INTERVAL = 60
seen_object_ids: Set[str] = set()


async def mark_as_failed(obj_id: str, reason: str):
    """
    [æ–°å¢] è¾…åŠ©å‡½æ•°ï¼šå½“å¤„ç†å‡ºé”™æ—¶ï¼Œå°†æ–°é—»æ ‡è®°ä¸º Noise (Tag 4)ï¼Œ
    é˜²æ­¢ç¨‹åºä¸‹æ¬¡é‡å¯æ—¶å¡åœ¨åŒä¸€ä¸ªé”™è¯¯çš„ ID ä¸Šã€‚
    """
    payload = {
        "objectId": obj_id,
        "newsTag": 4,  # 4 = å¤„ç†å¤±è´¥/å™ªéŸ³
        "summary": "Processing Failed",
        "analysis": f"System Error: {reason[:100]}"  # æˆªæ–­ä¸€ä¸‹é˜²æ­¢å¤ªé•¿
    }
    try:
        async with httpx.AsyncClient() as client:
            await client.post(UPDATE_API_URL, json=payload, headers=HEADERS, timeout=5.0)
            print(f"ğŸš« [ErrorHandler] å·²å°† ID {obj_id} æ ‡è®°ä¸º Tag 4 (Failed).")
    except Exception as e:
        print(f"âŒ [ErrorHandler] æ ‡è®°å¤±è´¥ ID {obj_id}: {e}")


async def fetch_crypto_news_from_api(client: httpx.AsyncClient, coin_type: int) -> List[Dict[str, Any]]:
    """
    è°ƒç”¨ fetchCryptoPanic æ¥å£è·å–æ–°é—»
    """
    # ä½¿ç”¨ UTC æ—¶é—´ï¼Œå¹¶ä¿ç•™ 24å°æ—¶çª—å£
    end_time = datetime.utcnow()
    start_time = end_time - timedelta(hours=24)

    # æ‰“å°ä¸€ä¸‹è¯·æ±‚çš„æ—¶é—´èŒƒå›´ï¼Œæ–¹ä¾¿è°ƒè¯•
    start_str = start_time.strftime("%Y-%m-%dT%H:%M:%S")
    end_str = end_time.strftime("%Y-%m-%dT%H:%M:%S")

    # print(f"ğŸ” [Debug API] Requesting Type {coin_type} | Range: {start_str} -> {end_str}")

    json_data = {
        "type": coin_type,
        "startTime": start_str,
        "endTime": end_str
    }

    try:
        req_start = time.time()
        response = await client.post(FETCH_API_URL, headers=HEADERS, json=json_data, timeout=15.0)
        duration = time.time() - req_start

        if response.status_code != 200:
            print(f"âš ï¸ [NewsCollector] API è¯·æ±‚å¤±è´¥ (Type {coin_type}) Code: {response.status_code}")
            return []

        data = response.json()
        # print(f"âœ… [Debug API] Type {coin_type} Success ({duration:.2f}s) | Items: {len(data)}")
        return data
    except Exception as e:
        print(f"âŒ [NewsCollector] è¯·æ±‚å¼‚å¸¸ (Type {coin_type}): {e}")
        return []


def parse_api_timestamp(time_str: str) -> float:
    """
    å°† API çš„æ—¶é—´å­—ç¬¦ä¸²è½¬æ¢ä¸º float æ—¶é—´æˆ³
    """
    if not time_str:
        return time.time()
    try:
        # é¢„å¤„ç†ï¼šæœ‰äº› API è¿”å›å¯èƒ½å¸¦ T ä½†ä¸å¸¦ Zï¼Œæˆ–è€…ä¸­é—´æœ‰ç©ºæ ¼
        clean_str = time_str.replace("Z", "").strip()

        # å°è¯•å¸¦ T çš„æ ¼å¼
        if "T" in clean_str:
            dt = datetime.strptime(clean_str, "%Y-%m-%dT%H:%M:%S")
        else:
            # å°è¯•ç©ºæ ¼æ ¼å¼
            dt = datetime.strptime(clean_str, "%Y-%m-%d %H:%M:%S")

        # å‡è®¾ API è¿”å›çš„æ˜¯ UTCï¼ŒåŠ ä¸Š timezone info
        dt = dt.replace(tzinfo=timezone.utc)
        return dt.timestamp()
    except Exception:
        # print(f"âš ï¸ [Time Parse Warning] Raw: {time_str} | Error: {e}")
        return time.time()


async def run_news_collector():
    print(f"Waiting 5s to start collector...")
    await asyncio.sleep(5)
    print(f"[NewsCollector]: ğŸš€ å¯åŠ¨æˆåŠŸ! (API: {FETCH_API_URL})")

    async with httpx.AsyncClient() as client:
        while True:
            try:
                loop_start = time.time()

                # 1. æ‹‰å–æ•°æ®
                btc_news = await fetch_crypto_news_from_api(client, 1)
                eth_news = await fetch_crypto_news_from_api(client, 2)

                all_news_items = []
                if isinstance(btc_news, list): all_news_items.extend(btc_news)
                if isinstance(eth_news, list): all_news_items.extend(eth_news)

                # 2. æ’åº (æœ€æ–°çš„ä¼˜å…ˆå¤„ç†)
                all_news_items.sort(
                    key=lambda x: parse_api_timestamp(x.get('time')),
                    reverse=True
                )

                total_items = len(all_news_items)
                processed_count = 0

                # print(f"ğŸ“Š [Loop Debug] Total Items fetched: {total_items}")

                for item in all_news_items:
                    obj_id = item.get('objectId')
                    current_tag = item.get('newsTag')

                    # è·³è¿‡å·²å¤„ç† (Tag != 0)
                    if current_tag is not None and current_tag != 0:
                        continue

                    # å†…å­˜å»é‡
                    if obj_id in seen_object_ids:
                        continue

                    if obj_id:
                        seen_object_ids.add(obj_id)
                        processed_count += 1

                        # --- å‡†å¤‡å¤„ç† ---
                        title = item.get('title') or "No Title"
                        target_url = item.get('link') or ""

                        # æ‰“å°è¯¦ç»†æ—¥å¿—ï¼šå¼€å§‹å¤„ç†
                        print(f"â³ [Processing Start] ID: {obj_id} | Title: {title[:30]}... | URL: {target_url[:40]}...")

                        raw_data = RawDataInput(
                            source=target_url,
                            timestamp=parse_api_timestamp(item.get('time')),
                            content=f"Title: {title}\nDescription: {item.get('description') or ''}",
                            object_id=obj_id
                        )

                        # --- æ ¸å¿ƒå¤„ç†é€»è¾‘ (ä¸²è¡Œ) ---
                        try:
                            task_start = time.time()

                            # è¿™é‡Œä¾ç„¶æ˜¯é˜»å¡çš„ awaitï¼Œå¤„ç†å®Œä¸€æ¡æ‰ä¼šèµ°ä¸‹ä¸€æ¡
                            await small_agent_graph.ainvoke({"raw_data": raw_data})

                            task_duration = time.time() - task_start
                            print(f"âœ… [Processing Done] ID: {obj_id} | Cost: {task_duration:.2f}s")

                            # ç¨ä½œä¼‘æ¯ï¼Œé˜²æ­¢æŠŠæ¥å£æ‰“æŒ‚
                            await asyncio.sleep(0.5)

                        except Exception as agent_e:
                            print(f"âŒ [Pipeline Error] ID: {obj_id} Failed!")
                            # æ‰“å°è¯¦ç»†å †æ ˆï¼Œæ–¹ä¾¿ä½ çœ‹å…·ä½“çš„æŠ¥é”™ä½ç½®
                            traceback.print_exc()

                            # ã€æ–°å¢ã€‘å…³é”®ï¼šæŠ¥é”™åå°è¯•æ ‡è®°ä¸º Failedï¼Œé¿å…æ­»å¾ªç¯
                            await mark_as_failed(obj_id, str(agent_e))

                # 3. æœ¬è½®æ€»ç»“
                loop_duration = time.time() - loop_start
                if processed_count > 0:
                    print(
                        f"ğŸš€ [NewsCollector] æœ¬è½®å¾ªç¯ç»“æŸï¼Œå…±å¤„ç† {processed_count} æ¡æ–°æ–°é—»ã€‚è€—æ—¶: {loop_duration:.2f}s")
                else:
                    # å¿ƒè·³æ—¥å¿—ï¼šè¯æ˜ç¨‹åºè¿˜åœ¨è·‘
                    print(f"ğŸ’“ [Heartbeat] No new items. (Sleeping {NEWS_POLL_INTERVAL}s)")

            except Exception as e:
                print(f"ğŸ”¥ [NewsCollector Global Error]: {e}")
                traceback.print_exc()
                await asyncio.sleep(10)

            await asyncio.sleep(NEWS_POLL_INTERVAL)


async def start_all_collectors():
    await asyncio.gather(
        run_news_collector(),
    )


if __name__ == "__main__":
    asyncio.run(start_all_collectors())