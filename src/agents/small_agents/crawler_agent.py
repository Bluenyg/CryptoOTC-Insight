# src/agents/small_agents/crawler_agent.py
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

# å®šä¹‰ä¸€ç»„é«˜ä¼˜å…ˆçº§çš„æ­£æ–‡é€‰æ‹©å™¨
# å¤§å¤šæ•°æ–°é—»ç½‘ç«™çš„å†…å®¹éƒ½åœ¨è¿™äº›æ ‡ç­¾é‡Œ
MAIN_CONTENT_SELECTORS = "article, main, .post-content, .entry-content, .article-body, #content"

# å®šä¹‰éœ€è¦å¼ºåˆ¶æ’é™¤çš„å™ªéŸ³é€‰æ‹©å™¨ (åŒ…æ‹¬ Cookie å¼¹çª—ã€ä¾§è¾¹æ ã€æ¨èé˜…è¯»)
EXCLUDED_SELECTORS = (
    "nav, footer, header, aside, script, style, noscript, "
    ".cookie-banner, .gdpr-banner, #onetrust-banner-sdk, "  # Cookie å¼¹çª—
    ".sidebar, .widget, .advertisement, .ad-container, "  # å¹¿å‘Šä¾§è¾¹æ 
    ".related-posts, .comments-area, .share-buttons"  # æ¨èå’Œè¯„è®º
)


async def run_crawler_agent(url: str) -> str | None:
    """
    ä½¿ç”¨ Crawl4AI æ™ºèƒ½æŠ“å–ç½‘é¡µæ ¸å¿ƒå†…å®¹ï¼Œè‡ªåŠ¨å»é™¤å¯¼èˆªå’Œå¼¹çª—å™ªéŸ³ã€‚
    """
    if not url or not url.startswith("http"):
        return None

    print(f"ğŸ•·ï¸ [Crawler] Intelligent Fetching: {url}")

    browser_config = BrowserConfig(
        headless=True,
        verbose=False,
        java_script_enabled=True,
        text_mode=True  # ä¼˜åŒ–æ–‡æœ¬æå–æ¨¡å¼
    )

    # é…ç½®è¿è¡Œå‚æ•°ï¼šæ ¸å¿ƒæ˜¯ css_selector å’Œ excluded_tags
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        # ã€å…³é”®ä¿®æ”¹ 1ã€‘æŒ‡å®šæŠ“å–èŒƒå›´ï¼šåªæŠ“å–é¡µé¢ä¸­çš„æ­£æ–‡åŒºåŸŸ
        # å¦‚æœç½‘ç«™åŒ…å« <article> æ ‡ç­¾ï¼ŒCrawl4AI å°†åªè¿”å›è¯¥æ ‡ç­¾å†…çš„ Markdown
        css_selector=MAIN_CONTENT_SELECTORS,

        # ã€å…³é”®ä¿®æ”¹ 2ã€‘æ’é™¤ CSS é€‰æ‹©å™¨ï¼šå¼ºåŠ›å»é™¤å¯¼èˆªå’Œæ— å…³å…ƒç´ 
        excluded_selector=EXCLUDED_SELECTORS,

        # ç§»é™¤å¤ªçŸ­çš„æ–‡æœ¬å— (é˜²æ­¢ä¿ç•™ 'Read more' è¿™ç§æŒ‰é’®æ–‡å­—)
        word_count_threshold=10,
    )

    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url=url,
                config=run_config
            )

            if result.success:
                # æ£€æŸ¥æŠ“å–ç»“æœæ˜¯å¦ä¸ºç©º (æœ‰æ—¶å€™é€‰æ‹©å™¨å¤ªä¸¥æ ¼å¯èƒ½å¯¼è‡´æŠ“ç©º)
                markdown_content = result.markdown

                # ã€å…œåº•ç­–ç•¥ã€‘å¦‚æœæŒ‡å®šé€‰æ‹©å™¨æŠ“ä¸åˆ°å†…å®¹ï¼ˆæ¯”å¦‚ç½‘ç«™ç»“æ„å¾ˆç‰¹æ®Šï¼‰ï¼Œåˆ™å°è¯•æŠ“å–å…¨æ–‡ä½†æ’é™¤å™ªéŸ³
                if not markdown_content or len(markdown_content) < 100:
                    print(f"âš ï¸ [Crawler] Main selector failed, falling back to body crawl for {url}")
                    fallback_config = CrawlerRunConfig(
                        cache_mode=CacheMode.BYPASS,
                        excluded_selector=EXCLUDED_SELECTORS,  # ä¾ç„¶ä¿æŒæ’é™¤å™ªéŸ³
                        word_count_threshold=20
                    )
                    fallback_result = await crawler.arun(url=url, config=fallback_config)
                    markdown_content = fallback_result.markdown

                # æˆªæ–­è¿‡é•¿å†…å®¹ï¼Œä¿ç•™å‰ 6000 å­—ç¬¦ï¼ˆè¶³ä»¥åŒ…å«æ ¸å¿ƒæ–°é—»ï¼‰
                print(f"âœ… [Crawler] Scraped length: {len(markdown_content)}")
                return markdown_content[:6000]
            else:
                print(f"[Crawler] Failed: {result.error_message}")
                return None

    except Exception as e:
        print(f"[Crawler] Error: {e}")
        return None


# æœ¬åœ°æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    async def test():
        # ä½¿ç”¨é“¾æ¥è¿›è¡Œæµ‹è¯•
        url = " https://www.cryptointelligence.co.uk/bitcoin-mirrors-2022-market-patterns-as-correlation-nears-100/"
        content = await run_crawler_agent(url)
        print("\n--- Final Cleaned Content ---\n")
        print(content)


    asyncio.run(test())