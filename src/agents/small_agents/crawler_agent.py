# src/agents/small_agents/crawler_agent.py
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

# å®šä¹‰ä¸€ç»„é«˜ä¼˜å…ˆçº§çš„æ­£æ–‡é€‰æ‹©å™¨
MAIN_CONTENT_SELECTORS = "article, main, .post-content, .entry-content, .article-body, #content"

# å®šä¹‰éœ€è¦å¼ºåˆ¶æ’é™¤çš„å™ªéŸ³é€‰æ‹©å™¨
EXCLUDED_SELECTORS = (
    "nav, footer, header, aside, script, style, noscript, "
    ".cookie-banner, .gdpr-banner, #onetrust-banner-sdk, "
    ".sidebar, .widget, .advertisement, .ad-container, "
    ".related-posts, .comments-area, .share-buttons"
)


async def run_crawler_agent(url: str) -> str | None:
    """
    ä½¿ç”¨ Crawl4AI æ™ºèƒ½æŠ“å–ç½‘é¡µæ ¸å¿ƒå†…å®¹ï¼Œè‡ªåŠ¨å»é™¤å¯¼èˆªå’Œå¼¹çª—å™ªéŸ³ã€‚
    [æ–°å¢] å¢åŠ  3 æ¬¡é‡è¯•æœºåˆ¶
    """
    if not url or not url.startswith("http"):
        return None

    print(f"ğŸ•·ï¸ [Crawler] Intelligent Fetching: {url}")

    browser_config = BrowserConfig(
        headless=True,
        verbose=False,
        java_script_enabled=True,
        text_mode=True
    )

    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        css_selector=MAIN_CONTENT_SELECTORS,
        excluded_selector=EXCLUDED_SELECTORS,
        word_count_threshold=10,
    )

    # [æ–°å¢] é‡è¯•å¾ªç¯
    max_retries = 3

    # æ³¨æ„ï¼šæˆ‘ä»¬å°† AsyncWebCrawler ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ”¾åœ¨é‡è¯•å¤–é¢ï¼Œå¤ç”¨æµè§ˆå™¨å®ä¾‹
    # å¦‚æœé‡åˆ°æµè§ˆå™¨å´©æºƒç­‰ä¸¥é‡é”™è¯¯ï¼Œå¯èƒ½éœ€è¦æŠŠå®ƒæ”¾é‡Œé¢ï¼Œä½†é€šå¸¸è¿™æ ·å¤Ÿäº†
    try:
        async with AsyncWebCrawler(config=browser_config) as crawler:
            for attempt in range(max_retries):
                try:
                    result = await crawler.arun(url=url, config=run_config)

                    if result.success:
                        markdown_content = result.markdown

                        # ã€å…œåº•ç­–ç•¥ã€‘
                        if not markdown_content or len(markdown_content) < 100:
                            print(f"âš ï¸ [Crawler] Main selector failed, trying fallback... ({url})")
                            fallback_config = CrawlerRunConfig(
                                cache_mode=CacheMode.BYPASS,
                                excluded_selector=EXCLUDED_SELECTORS,
                                word_count_threshold=20
                            )
                            fallback_result = await crawler.arun(url=url, config=fallback_config)
                            markdown_content = fallback_result.markdown

                        if markdown_content and len(markdown_content) >= 50:
                            print(f"âœ… [Crawler] Scraped length: {len(markdown_content)}")
                            return markdown_content[:6000]
                        else:
                            raise ValueError("Content too short or empty after fallback")
                    else:
                        raise ValueError(f"Crawl failed: {result.error_message}")

                except Exception as e:
                    if attempt == max_retries - 1:
                        print(f"âŒ [Crawler] Failed after {max_retries} attempts: {e}")
                        return None

                    print(f"âš ï¸ [Crawler] Retry ({attempt + 1}/{max_retries}) for {url}: {e}")
                    await asyncio.sleep(2)  # ç­‰å¾…2ç§’é‡è¯•

    except Exception as e:
        print(f"âŒ [Crawler] Browser Init Error: {e}")
        return None


# æœ¬åœ°æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    async def test():
        # ä½¿ç”¨é“¾æ¥è¿›è¡Œæµ‹è¯•
        url = " https://www.cryptointelligence.co.uk/bitcoin-mirrors-2022-market-patterns-as-correlation-nears-100/"
        content = await run_crawler_agent(url)
        print("\n--- Final Cleaned Content ---\n")
        print(content)


    asyncio.run(test())