# src/agents/small_agents/pipeline.py
from typing_extensions import TypedDict
from typing import Literal, Optional
from langgraph.graph import StateGraph, END
import httpx
import asyncio
from datetime import datetime, timedelta

from src.schemas.data_models import RawDataInput, ProcessedData
from .filter_agent import run_filter_agent
from .nlp_agent import run_nlp_agent
from .crawler_agent import run_crawler_agent

# --- é…ç½® ---
UPDATE_API_URL = "http://api.ibyteai.com:15008/10Ai/dataCenter/crypto/updatePanicNews"
# [æ–°å¢ž] è¯»å–æŽ¥å£ï¼Œç”¨äºŽå›žè¯»éªŒè¯
FETCH_API_URL = "http://api.ibyteai.com:15008/10Ai/dataCenter/crypto/fetchCryptoPanic"
HEADERS = {'Content-Type': 'application/json', 'User-Agent': 'Mozilla/5.0'}


# --- 1. State ---
class SmallAgentState(TypedDict):
    raw_data: RawDataInput
    processed_data: Optional[ProcessedData]
    is_relevant: bool
    full_content: Optional[str]


# --- 2. Nodes ---

async def filter_node(state: SmallAgentState) -> dict:
    # ... (ä¿æŒä¸å˜) ...
    raw_data = state['raw_data']
    is_relevant = await run_filter_agent(raw_data)
    return {"is_relevant": is_relevant, "raw_data": raw_data, "full_content": raw_data.content}


async def crawler_node(state: SmallAgentState) -> dict:
    # ... (ä¿æŒä¸å˜) ...
    raw_data = state['raw_data']
    target_url = raw_data.source
    scraped_text = None
    if target_url and target_url.startswith("http"):
        print(f"ðŸ•·ï¸ [Crawler] Fetching: {target_url}")
        scraped_text = await run_crawler_agent(target_url)
    if scraped_text:
        print(f"âœ… [Crawler] Success ({len(scraped_text)} chars)")
        enriched_content = f"ã€Web Scraped Contentã€‘\n{scraped_text}\n\nã€Original Metaã€‘\n{raw_data.content}"
        raw_data.content = enriched_content
        return {"full_content": scraped_text, "raw_data": raw_data}
    else:
        print(f"âš ï¸ [Crawler] Skipped or Failed: {target_url}")
        return {}


async def analysis_node(state: SmallAgentState) -> dict:
    # ... (ä¿æŒä¸å˜) ...
    raw_data = state['raw_data']
    processed_data = await run_nlp_agent(raw_data)
    if processed_data:
        processed_data.object_id = raw_data.object_id
        return {"processed_data": processed_data}
    else:
        return {"is_relevant": False}


# --- [æ–°å¢ž] éªŒè¯è¾…åŠ©å‡½æ•° ---
async def verify_db_write(client: httpx.AsyncClient, object_id: str, expected_tag: int) -> bool:
    """
    è¾…åŠ©å‡½æ•°ï¼šå°è¯•ä»Žæ•°æ®åº“å›žè¯»åˆšæ‰å†™å…¥çš„æ•°æ®ï¼ŒéªŒè¯ Tag æ˜¯å¦æ›´æ–°æˆåŠŸã€‚
    """
    # èŽ·å–è¿‡åŽ» 24 å°æ—¶çš„æ•°æ®èŒƒå›´
    end_time = datetime.now()
    start_time = end_time - timedelta(hours=24)

    # å› ä¸ºä¸çŸ¥é“è¯¥æ–°é—»æ˜¯ BTC(1) è¿˜æ˜¯ ETH(2)ï¼Œæˆ‘ä»¬ä¸¤ä¸ªéƒ½æŸ¥ä¸€ä¸‹
    for coin_type in [1, 2]:
        try:
            payload = {
                "type": coin_type,
                "startTime": start_time.strftime("%Y-%m-%d %H:%M:%S"),
                "endTime": end_time.strftime("%Y-%m-%d %H:%M:%S")
            }
            # è°ƒç”¨ fetch æŽ¥å£
            response = await client.post(FETCH_API_URL, json=payload, headers=HEADERS, timeout=10.0)

            if response.status_code == 200:
                items = response.json()
                # åœ¨åˆ—è¡¨ä¸­æŸ¥æ‰¾æˆ‘ä»¬çš„ object_id
                target_item = next((item for item in items if str(item.get('objectId')) == str(object_id)), None)

                if target_item:
                    # æ‰¾åˆ°äº†ï¼æ£€æŸ¥å­—æ®µ
                    # å…¼å®¹è¯»å– newsTag, newTag, tag
                    actual_tag = target_item.get('newsTag')
                    if actual_tag is None: actual_tag = target_item.get('newTag')
                    if actual_tag is None: actual_tag = target_item.get('tag')

                    # å¼ºè½¬ int å¯¹æ¯”
                    try:
                        actual_tag = int(actual_tag)
                    except:
                        actual_tag = 0

                    print(
                        f"   ðŸ” [Verify] Found ID {object_id} in Type {coin_type}. DB Tag: {actual_tag} | Expected: {expected_tag}")

                    if actual_tag == expected_tag:
                        return True
                    else:
                        print(f"   âš ï¸ [Verify Mismatch] DB has {actual_tag}, expected {expected_tag}")
                        return False
        except Exception as e:
            print(f"   âš ï¸ [Verify Error] Type {coin_type}: {e}")

    print(f"   âŒ [Verify Failed] Object ID {object_id} not found in recent API data.")
    return False


async def db_write_node(state: SmallAgentState):
    """å†™å…¥èŠ‚ç‚¹ (åŒ…å«å›žè¯»éªŒè¯é€»è¾‘)"""
    data = state['processed_data']
    final_content = state.get('full_content') or ""

    if data is None: return {}

    tag_map = {"BULLISH": 1, "NEUTRAL": 2, "BEARISH": 3}
    sentiment_str = data.sentiment.value if hasattr(data.sentiment, 'value') else str(data.sentiment)
    tag_value = tag_map.get(sentiment_str, 2)
    impact_str = data.market_impact.value if hasattr(data.market_impact, 'value') else str(data.market_impact)

    # æž„é€ å…¨å­—æ®µ Payload
    payload = {
        "objectId": data.object_id,
        "tag": tag_value,
        "newsTag": tag_value,  # åŒé‡ä¿é™©
        "newTag": tag_value,  # ä¸‰é‡ä¿é™©
        "summary": data.summary,
        "analysis": f"Impact:{impact_str}|Score:{data.long_short_score}",
        "content": final_content
    }

    try:
        async with httpx.AsyncClient() as client:
            # 1. æ‰§è¡Œå†™å…¥
            response = await client.post(UPDATE_API_URL, json=payload, headers=HEADERS, timeout=10.0)

            if response.status_code == 200:
                print(f"âœ… [Pipeline] Write OK. Tag:{tag_value}. Now Verifying...")

                # 2. ç­‰å¾… 1 ç§’ï¼Œç»™æ•°æ®åº“ä¸€ç‚¹æ—¶é—´è½ç›˜
                await asyncio.sleep(1.0)

                # 3. æ‰§è¡Œå›žè¯»éªŒè¯
                is_verified = await verify_db_write(client, data.object_id, tag_value)

                if is_verified:
                    print(f"ðŸŽ‰ [Pipeline] DOUBLE CHECK PASSED! Data is strictly consistent.")
                else:
                    print(f"ðŸ’Š [Pipeline] Write 200 OK, but Read-Back check failed (Might be latency).")
            else:
                print(f"âŒ [Pipeline] API Error: {response.status_code}")

    except Exception as e:
        print(f"âŒ [Pipeline] Connection Error: {e}")

    return {}


# --- (log_noise_node, decide_to_process, create_small_agent_graph ä¿æŒä¸å˜) ---
# ... (å¤åˆ¶æ‚¨åŽŸæ¥çš„ä»£ç å³å¯) ...
async def log_noise_node(state: SmallAgentState):
    """è®°å½•å™ªéŸ³èŠ‚ç‚¹"""
    raw_data = state['raw_data']
    payload = {
        "objectId": raw_data.object_id,
        "newsTag": 4,
        "summary": "Filtered as Noise",
        "analysis": "Status:Ignored"
    }
    try:
        async with httpx.AsyncClient() as client:
            await client.post(UPDATE_API_URL, json=payload, headers=HEADERS, timeout=5.0)
            print(f"ðŸ—‘ï¸ [Pipeline] Marked as NOISE: {raw_data.object_id}")
    except Exception:
        pass
    return {}


def decide_to_process(state: SmallAgentState) -> Literal["crawler", "log_noise"]:
    if state["is_relevant"]:
        return "crawler"
    else:
        return "log_noise"


def create_small_agent_graph():
    graph = StateGraph(SmallAgentState)
    graph.add_node("filter", filter_node)
    graph.add_node("crawler", crawler_node)
    graph.add_node("analysis", analysis_node)
    graph.add_node("db_write", db_write_node)
    graph.add_node("log_noise", log_noise_node)

    graph.set_entry_point("filter")

    graph.add_conditional_edges("filter", decide_to_process, {
        "crawler": "crawler",
        "log_noise": "log_noise"
    })

    graph.add_edge("crawler", "analysis")
    graph.add_edge("analysis", "db_write")
    graph.add_edge("db_write", END)
    graph.add_edge("log_noise", END)

    return graph.compile()


small_agent_graph = create_small_agent_graph()